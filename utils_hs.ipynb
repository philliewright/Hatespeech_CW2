{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPCMvEKzb1Y+7O6PXOM2fvH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/philliewright/Hatespeech_CW2/blob/main/utils_hs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I2JCVS6SXLdq",
        "outputId": "ef992c78-be4c-4c2c-b011-ecf427327c55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/drive/My Drive/Projects_Portfolio/hatespeech_detection/utils_hs.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile \"/content/drive/My Drive/Projects_Portfolio/hatespeech_detection/utils_hs.py\"\n",
        "\n",
        "# loading neccesary libraries\n",
        "\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "import re\n",
        "import pandas as pd\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, ENGLISH_STOP_WORDS\n",
        "from sklearn.decomposition import LatentDirichletAllocation, NMF\n",
        "import pyLDAvis\n",
        "import numpy as np\n",
        "import time\n",
        "from gensim.models import CoherenceModel\n",
        "from gensim.corpora.dictionary import Dictionary\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "import seaborn as sns\n",
        "#from google.colab import drive\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
        "\n",
        "def load_csv(file_path):\n",
        "    try:\n",
        "        data = pd.read_csv(file_path)\n",
        "        if data.empty:\n",
        "            raise ValueError(\"The CSV file is empty\")\n",
        "        return data\n",
        "    except FileNotFoundError:\n",
        "        raise FileNotFoundError(\"The specified file does not exist\")\n",
        "    except Exception as e:\n",
        "        raise Exception(f\"An unknown error occurred: {e}\")\n",
        "\n",
        "def report_missing_values(data):\n",
        "    \"\"\" Checks for missing values\"\"\"\n",
        "    missing_values = data.isnull().sum()\n",
        "    print(\"Missing values in each column:\")\n",
        "    for column, missing_value_count in missing_values.items():\n",
        "        print(f\"{column}: {missing_value_count}\")\n",
        "\n",
        "def tokenise_text(data):\n",
        "    \"\"\"\n",
        "    Tokenise the text in the clean_text column\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # label changed to a float when I imported it, so changing it back\n",
        "        data['oh_label'] = data['oh_label'].astype(int)\n",
        "        data['clean_text'] = data['clean_text'].astype(str)\n",
        "\n",
        "        # Begin by tokenizing the words\n",
        "        # also lowercase all words for consistency\n",
        "        data['tokens'] = data['clean_text'].apply(lambda x: [word.lower() for word in x.split()]) #ensures all text is lowercase\n",
        "        print(\"Tokenisation successful\")\n",
        "        return data\n",
        "    except Exception as e:\n",
        "        print(f\"Tokenisation error: {e}\")\n",
        "        return None\n",
        "\n",
        "def lemmatize_text(data):\n",
        "    \"\"\"\n",
        "    Lemmatises the tesxt data\n",
        "    \"\"\"\n",
        "    try:\n",
        "        lemm = WordNetLemmatizer() #using the inbuilt lemmatisation function\n",
        "\n",
        "    # Lemmatize all words\n",
        "        data['lemmatized'] = data['tokens'].apply(lambda x: [lemm.lemmatize(word) for word in x])\n",
        "        print(\"Lemmatisation successful\")\n",
        "        return data\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during lemmatisation: {e}\")\n",
        "        return data\n",
        "\n",
        "def word_frequency_analysis(data):\n",
        "    \"\"\"\n",
        "    calculates the most common words in the data\n",
        "    \"\"\"\n",
        "    words_1 = data[data.oh_label == 1]['lemmatized']\n",
        "    words_0 = data[data.oh_label == 0]['lemmatized']\n",
        "\n",
        "    _1_words = Counter(word for words in words_1 for word in str(words).split())\n",
        "    _0_words = Counter(word for words in words_0 for word in str(words).split())\n",
        "\n",
        "    print(\"Most common words for oh_label = 1:\")\n",
        "    print(_1_words.most_common(50))\n",
        "\n",
        "    print(\"Most common words for oh_label = 0:\")\n",
        "    print(_0_words.most_common(50))\n",
        "\n",
        "# Function to remove numbers from a list of words\n",
        "def remove_numbers(word_list):\n",
        "    \"\"\"\n",
        "    removees any numbers from the text\n",
        "    \"\"\"\n",
        "    return [word for word in word_list if not bool(re.search(r'\\d', word))]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Function to remove URLs from a list of words\n",
        "def remove_urls(word_list):\n",
        "    \"\"\"\n",
        "    Removes any URLs from the text\n",
        "    \"\"\"\n",
        "    return [word for word in word_list if not (word.startswith('http') or word.startswith('www'))]\n",
        "\n",
        "def bag_of_words(data, column_name, max_features=5000):\n",
        "    \"\"\"\n",
        "    Creates a Bag of Words representation of the text data\n",
        "    \"\"\"\n",
        "    # Joining the lemmatised words to form a string, as CountVectorizer requires string input\n",
        "    data['string_lemmatized'] = data[column_name].apply(' '.join)\n",
        "\n",
        "    # Creates the CountVectorizer and fits it to the data\n",
        "    vectorizer = CountVectorizer(max_features=max_features)\n",
        "    X_bag_words = vectorizer.fit_transform(data['string_lemmatized'])\n",
        "\n",
        "    # Converts to a DataFrame\n",
        "    df_bag_words = pd.DataFrame(X_bag_words.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "\n",
        "    return df_bag_words , vectorizer\n",
        "\n",
        "\n",
        "def tfidf(data, column_name, max_features=5000):\n",
        "    \"\"\"\n",
        "    Creates a TF-IDF representation of the text data\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Joining the lemmatized words to form a string since TfidfVectorizer requires string input\n",
        "        data['string_lemmatized'] = data[column_name].apply(' '.join)\n",
        "\n",
        "        # Create the TfidfVectorizer and fit it to the data\n",
        "        tfidf_vectorizer = TfidfVectorizer(max_features=max_features)\n",
        "        X_TFIDF = tfidf_vectorizer.fit_transform(data['string_lemmatized'])\n",
        "\n",
        "        # Convert the result to a DataFrame\n",
        "        df_TFIDF = pd.DataFrame(X_TFIDF.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
        "\n",
        "        return df_TFIDF, tfidf_vectorizer\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during TF-IDF vectorization: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def visualise_topics(data, vectorizer, lda_model):\n",
        "    \"\"\"\n",
        "    Visualiseing topics using pyLDAvis\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # calculate the topic-term distribution\n",
        "    topic_term_dists = lda_model.components_ / lda_model.components_.sum(axis=1)[:, None]\n",
        "\n",
        "    # calculate the document-topic distribution\n",
        "    doc_topic_dists = lda_model.transform(data)\n",
        "\n",
        "    # document lengths\n",
        "    doc_lengths = data.sum(axis=1).tolist()\n",
        "\n",
        "    # frequencies of vocab and terms\n",
        "    vocab = vectorizer.get_feature_names_out()\n",
        "    term_frequency = data.sum(axis=0).values.ravel()\n",
        "\n",
        "    # Visualisation\n",
        "    vis = pyLDAvis.prepare(\n",
        "        topic_term_dists=topic_term_dists,\n",
        "        doc_topic_dists=doc_topic_dists,\n",
        "        doc_lengths=doc_lengths,\n",
        "        vocab=vocab,\n",
        "        term_frequency=term_frequency\n",
        "    )\n",
        "\n",
        "    return vis\n",
        "\n",
        "def assign_topic_to_docs(doc_topic_dists):\n",
        "    \"\"\"Assigning the most likely topic to each document\"\"\"\n",
        "    return np.argmax(doc_topic_dists, axis=1)\n",
        "\n",
        "def calculate_oh_label_proportion(data, assigned_topics):\n",
        "    \"\"\"Calculates the proportion of comments with oh_label=1 for each topic..\"\"\"\n",
        "    topic_oh_label_props = {}\n",
        "    for topic in np.unique(assigned_topics):\n",
        "        indices = np.where(assigned_topics == topic)\n",
        "        proportion = data.iloc[indices]['oh_label'].mean()\n",
        "        topic_oh_label_props[topic] = proportion\n",
        "    return topic_oh_label_props\n",
        "\n",
        "# Gets the top n words for each topic\n",
        "def get_topics(lda_model, vectorizer, top_n=10):\n",
        "    \"\"\"top_n words for each topic in the lda\"\"\"\n",
        "    topics = []\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "    for topic_idx, topic in enumerate(lda_model.components_):\n",
        "        top_features_idx = topic.argsort()[-top_n:][::-1]\n",
        "        topics.append([feature_names[i] for i in top_features_idx])\n",
        "    return topics\n",
        "\n",
        "\n",
        "# Function to get the top N words from each topic\n",
        "def get_top_words_from_topic(model, feature_names, n_top_words=50):\n",
        "    top_words = {}\n",
        "    for topic_idx, topic in enumerate(model.components_):\n",
        "        top_words[topic_idx] = [feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]\n",
        "    return top_words\n",
        "\n",
        "# Function to get contextual keywords for each topic based on hate speech terms\n",
        "\n",
        "def get_contextual_keywords(texts, hate_terms, assigned_topics, window=5):\n",
        "    contextual_keywords = {}\n",
        "    for topic in set(assigned_topics):\n",
        "        contextual_keywords[topic] = Counter()\n",
        "\n",
        "    for text, topic in zip(texts, assigned_topics):\n",
        "        words = text\n",
        "        for i, word in enumerate(words):\n",
        "            if word in hate_terms[topic]:\n",
        "                # Get surrounding words\n",
        "                start = max(0, i - window)\n",
        "                end = min(len(words), i + window + 1)\n",
        "                surrounding_words = words[start:i] + words[i + 1:end]\n",
        "\n",
        "                # Exclude hates speech terms from surrounding_words\n",
        "                surrounding_words = [w for w in surrounding_words if w not in hate_terms[topic]]\n",
        "\n",
        "                contextual_keywords[topic].update(surrounding_words)\n",
        "\n",
        "    return contextual_keywords\n",
        "\n",
        "def sentiment_analysis(data):\n",
        "    # Use the Sentiment Intensity Analyzer\n",
        "    intensity_analyser = SentimentIntensityAnalyzer()\n",
        "\n",
        "    # Convert the lemmatised words back to strings, needed for VADER to analyse\n",
        "    data['lemmatized_string'] = data['lemmatized_clean'].apply(' '.join)\n",
        "\n",
        "    # Apply VADER analysis\n",
        "    data['sentiment_scores'] = data['lemmatized_string'].apply(lambda x: intensity_analyser.polarity_scores(x)['compound'])\n",
        "\n",
        "    # Classifying the sentiments\n",
        "    data['sentiment'] = data['sentiment_scores'].apply(lambda x: 'Positive' if x >= 0.05 else ('Neutral' if x > -0.05 else 'Negative'))\n",
        "\n",
        "    # Pivot table to analyse the sentiment scores wrt the oh_label\n",
        "    pivot_table = pd.pivot_table(data, values='sentiment_scores', index=['sentiment'], columns=['oh_label'], aggfunc='count', fill_value=0)\n",
        "\n",
        "    # Percentage of each sentiment category for each oh_label\n",
        "    total_counts_per_label = pivot_table.sum(axis=0)\n",
        "    pivot_table_percentage = (pivot_table / total_counts_per_label) * 100\n",
        "    pivot_table_percentage = pivot_table_percentage.round(2)\n",
        "\n",
        "    return data, pivot_table_percentage\n",
        "\n",
        "\n"
      ]
    }
  ]
}
