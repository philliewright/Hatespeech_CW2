{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN4tiKPeM85zZuz3JX+8Gj+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/philliewright/Hatespeech_CW2/blob/main/utils_hs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I2JCVS6SXLdq",
        "outputId": "76b0ff16-3ac6-4864-ed75-1e4964c9e0a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/drive/My Drive/Projects_Portfolio/hatespeech_detection/utils_hs.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile \"/content/drive/My Drive/Projects_Portfolio/hatespeech_detection/utils_hs.py\"\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "def load_csv(file_path):\n",
        "    try:\n",
        "        data = pd.read_csv(file_path)\n",
        "        if data.empty:\n",
        "            raise ValueError(\"The CSV file is empty\")\n",
        "        return data\n",
        "    except FileNotFoundError:\n",
        "        raise FileNotFoundError(\"The specified file does not exist\")\n",
        "    except Exception as e:\n",
        "        raise Exception(f\"An unknown error occurred: {e}\")\n",
        "\n",
        "def report_missing_values(data):\n",
        "    \"\"\" Checks for missing values\"\"\"\n",
        "    missing_values = data.isnull().sum()\n",
        "    print(\"Missing values in each column:\")\n",
        "    for column, missing_value_count in missing_values.items():\n",
        "        print(f\"{column}: {missing_value_count}\")\n",
        "\n",
        "def tokenise_text(data):\n",
        "    \"\"\"\n",
        "    Tokenise the text in the clean_text column\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # label changed to a float when I imported it, so changing it back\n",
        "        data['oh_label'] = data['oh_label'].astype(int)\n",
        "        data['clean_text'] = data['clean_text'].astype(str)\n",
        "\n",
        "        # Begin by tokenizing the words\n",
        "        # also lowercase all words for consistency\n",
        "        data['tokens'] = data['clean_text'].apply(lambda x: [word.lower() for word in x.split()]) #ensures all text is lowercase\n",
        "        print(\"Tokenisation successful\")\n",
        "        return data\n",
        "    except Exception as e:\n",
        "        print(f\"Tokenisation error: {e}\")\n",
        "        return None\n",
        "\n",
        "def lemmatize_text(data):\n",
        "    \"\"\"\n",
        "    Lemmatises the tesxt data\n",
        "    \"\"\"\n",
        "    try:\n",
        "        lemm = WordNetLemmatizer() #using the inbuilt lemmatisation function\n",
        "\n",
        "    # Lemmatize all words\n",
        "        data['lemmatized'] = data['tokens'].apply(lambda x: [lemm.lemmatize(word) for word in x])\n",
        "        print(\"Lemmatisation successful\")\n",
        "        return data\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during lemmatisation: {e}\")\n",
        "        return None\n",
        "\n",
        "def word_frequency_analysis(data):\n",
        "    \"\"\"\n",
        "    calculates the most common words in the data\n",
        "    \"\"\"\n",
        "    words_1 = data[data.oh_label == 1]['lemmatized']\n",
        "    words_0 = data[data.oh_label == 0]['lemmatized']\n",
        "\n",
        "    _1_words = Counter(word for words in words_1 for word in str(words).split())\n",
        "    _0_words = Counter(word for words in words_0 for word in str(words).split())\n",
        "\n",
        "    print(\"Most common words for oh_label = 1:\")\n",
        "    print(_1_words.most_common(50))\n",
        "\n",
        "    print(\"Most common words for oh_label = 0:\")\n",
        "    print(_0_words.most_common(50))\n",
        "\n",
        "# Function to remove numbers from a list of words\n",
        "def remove_numbers(word_list):\n",
        "    \"\"\"\n",
        "    removees any numbers from the text\n",
        "    \"\"\"\n",
        "    return [word for word in word_list if not bool(re.search(r'\\d', word))]\n",
        "\n",
        "all_data['lemmatized_no_numbers'] = all_data['lemmatized'].apply(remove_numbers)\n",
        "\n",
        "\n",
        "\n",
        "# Function to remove URLs from a list of words\n",
        "def remove_urls(word_list):\n",
        "    \"\"\"\n",
        "    Removes any URLs from the text\n",
        "    \"\"\"\n",
        "    return [word for word in word_list if not (word.startswith('http') or word.startswith('www'))]\n",
        "\n",
        "all_data['lemmatized_clean'] = all_data['lemmatized_no_numbers'].apply(remove_urls)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4r19X1ibqc9",
        "outputId": "d32b7465-d742-4970-9ae5-80069922c70d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JsSZwZYOaSTI"
      },
      "execution_count": 19,
      "outputs": []
    }
  ]
}