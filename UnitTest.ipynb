{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e0632e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f02ae31",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def load_csv(file_path):\n",
    "    try:\n",
    "        data = pd.read_csv(file_path)\n",
    "        if data.empty:\n",
    "            raise ValueError(\"The CSV file is empty\")\n",
    "        return data\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(\"The specified file does not exist\")\n",
    "    except pd.errors.EmptyDataError:\n",
    "        raise ValueError(\"No data in the CSV file\")\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"An unknown error occurred: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "def tokenise_text(data):\n",
    "    \"\"\"\n",
    "    Tokenise the text in the clean_text column \n",
    "    \"\"\"\n",
    "    try:\n",
    "        # import changed oh_label into a float so changing it back\n",
    "        data['oh_label'] = data['oh_label'].astype(int)\n",
    "        data['clean_text'] = data['clean_text'].astype(str)\n",
    "    \n",
    "        # Begin by tokenizing the words\n",
    "        # also lowercase all words for consistency\n",
    "        data['tokens'] = data['clean_text'].apply(lambda x: [word.lower() for word in x.split()])\n",
    "        print(\"Tokenisation successful.\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"Tokenisation error: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "def lemmatize_text(data):\n",
    "    try:\n",
    "        lemm = WordNetLemmatizer()\n",
    "\n",
    "    # Lemmatize all words\n",
    "        data['lemmatized'] = data['tokens'].apply(lambda x: [lemm.lemmatize(word) for word in x])\n",
    "        print(\"Lemmatisation successful\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during lemmatisation: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "def word_frequency_analysis(data):\n",
    "    words_1 = data[data.oh_label == 1]['lemmatized']\n",
    "    words_0 = data[data.oh_label == 0]['lemmatized']\n",
    "\n",
    "    _1_words = Counter(word for words in words_1 for word in str(words).split())\n",
    "    _0_words = Counter(word for words in words_0 for word in str(words).split())\n",
    "\n",
    "    print(\"Most common words for oh_label = 1:\")\n",
    "    print(_1_words.most_common(50))\n",
    "\n",
    "    print(\"Most common words for oh_label = 0:\")\n",
    "    print(_0_words.most_common(50))\n",
    "\n",
    "\n",
    "def bag_of_words(data, column_name, max_features=5000):\n",
    "    \"\"\"\n",
    "    Function to create a Bag of Words representation of the text data\n",
    "    \"\"\"\n",
    "    # Joining the lemmatized words to form a string since CountVectorizer requires string input\n",
    "    data['string_lemmatized'] = data[column_name].apply(' '.join)\n",
    "\n",
    "    # Create the CountVectorizer and fit it to the data\n",
    "    vectorizer = CountVectorizer(max_features=max_features)  \n",
    "    X_bag_words = vectorizer.fit_transform(data['string_lemmatized'])\n",
    "\n",
    "    # Convert the result to a DataFrame\n",
    "    df_bag_words = pd.DataFrame(X_bag_words.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "    \n",
    "    return df_bag_words , vectorizer\n",
    "\n",
    "\n",
    "def tfidf(data, column_name, max_features=5000):\n",
    "    \"\"\"\n",
    "    Function to create a TF-IDF representation of the text data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Joining the lemmatized words to form a string since TfidfVectorizer requires string input\n",
    "        data['string_lemmatized'] = data[column_name].apply(' '.join)\n",
    "        \n",
    "        # Create the TfidfVectorizer and fit it to the data\n",
    "        tfidf_vectorizer = TfidfVectorizer(max_features=max_features)  \n",
    "        X_TFIDF = tfidf_vectorizer.fit_transform(data['string_lemmatized'])\n",
    "\n",
    "        # Convert the result to a DataFrame\n",
    "        df_TFIDF = pd.DataFrame(X_TFIDF.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "        \n",
    "        return df_TFIDF, tfidf_vectorizer\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during TF-IDF vectorization: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "def apply_lda(data, n_components=10, random_state=42):\n",
    "    \"\"\"\n",
    "    Function to apply Latent Dirichlet Allocation (LDA) on a Bag of Words representation of the text data.\n",
    "\n",
    "    \"\"\"\n",
    "    # Create the LDA model and fit it to the data\n",
    "    lda_model = LatentDirichletAllocation(n_components=n_components, random_state=random_state) \n",
    "    lda_model.fit(data)\n",
    "    \n",
    "    return lda_model\n",
    "\n",
    "\n",
    "\n",
    "def get_topics(lda_model, vectorizer, top_n=10):\n",
    "    \"\"\"top_n words for each topic in the lda\"\"\"\n",
    "    topics = []\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    for topic_idx, topic in enumerate(lda_model.components_):\n",
    "        top_features_idx = topic.argsort()[-top_n:][::-1]\n",
    "        topics.append([feature_names[i] for i in top_features_idx])\n",
    "    return topics\n",
    "\n",
    "\n",
    "def sentiment_analysis(data):\n",
    "    # Instantiate the Sentiment Intensity Analyzer\n",
    "    intensity_analyser = SentimentIntensityAnalyzer()\n",
    "    \n",
    "    # Convert the lemmatized words back to strings for VADER to analyze\n",
    "    data['lemmatized_string'] = data['lemmatized_clean'].apply(' '.join)\n",
    "\n",
    "    # Apply VADER analysis to compute sentiment scores\n",
    "    data['sentiment_scores'] = data['lemmatized_string'].apply(lambda x: intensity_analyser.polarity_scores(x)['compound'])\n",
    "\n",
    "    # Classify the sentiment based on the computed scores\n",
    "    data['sentiment'] = data['sentiment_scores'].apply(lambda x: 'Positive' if x >= 0.05 else ('Neutral' if x > -0.05 else 'Negative'))\n",
    "\n",
    "    # Create a pivot table to analyze the sentiment scores with respect to the oh_label\n",
    "    pivot_table = pd.pivot_table(data, values='sentiment_scores', index=['sentiment'], columns=['oh_label'], aggfunc='count', fill_value=0)\n",
    "\n",
    "    # Calculate the percentage of each sentiment category for each oh_label\n",
    "    total_counts_per_label = pivot_table.sum(axis=0)\n",
    "    pivot_table_percentage = (pivot_table / total_counts_per_label) * 100\n",
    "    pivot_table_percentage = pivot_table_percentage.round(2)\n",
    "    \n",
    "    return data, pivot_table_percentage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64e6fd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestFunctions(unittest.TestCase):\n",
    "    \n",
    "    def test_load_csv(self):\n",
    "        # Test loading a valid CSV file\n",
    "        data = load_csv('./output_all_data.csv')\n",
    "        self.assertIsInstance(data, pd.DataFrame)\n",
    "        \n",
    "        # Test loading a non-existing file (should raise FileNotFoundError)\n",
    "        with self.assertRaises(FileNotFoundError):\n",
    "            load_csv('fake_file.csv')\n",
    "        \n",
    "\n",
    "    \n",
    "    def test_tokenize_text(self):\n",
    "\n",
    "        data = pd.DataFrame({'oh_label': [1.0, 0.0], 'clean_text': ['This is a', 'Unit Test Prog']})\n",
    "        result = tokenise_text(data)\n",
    "        self.assertIn('tokens', result.columns)\n",
    "        \n",
    "    def test_lemmatize_text(self):\n",
    "        # Assuming data is a DataFrame with a 'tokens' column\n",
    "        data = pd.DataFrame({'tokens': [['this', 'is', 'a'], ['unit', 'test', 'prog']]})\n",
    "        result = lemmatize_text(data)\n",
    "        self.assertIn('lemmatized', result.columns)\n",
    "        \n",
    "    def test_word_frequency_analysis(self):\n",
    "        data = pd.DataFrame({\n",
    "            'oh_label': [1, 0],\n",
    "            'lemmatized': [['this', 'is','a'], ['unit', 'test','prog']]\n",
    "        })\n",
    "        word_frequency_analysis(data)\n",
    "        \n",
    "    def test_bag_of_words(self):\n",
    "        data = pd.DataFrame({'lemmatized': [['this', 'is','a'], ['unit', 'test','prog']]})\n",
    "        result, vectorizer = bag_of_words(data, 'lemmatized')\n",
    "        self.assertIsInstance(result, pd.DataFrame)\n",
    "        self.assertIsInstance(vectorizer, CountVectorizer)\n",
    "        \n",
    "    def test_tfidf(self):\n",
    "        data = pd.DataFrame({'lemmatized': [['this', 'is','a'], ['unit', 'test','prog']]})\n",
    "        result, tfidf_vectorizer = tfidf(data, 'lemmatized')\n",
    "        self.assertIsInstance(result, pd.DataFrame)\n",
    "        self.assertIsInstance(tfidf_vectorizer, TfidfVectorizer)\n",
    "        \n",
    "    def test_apply_lda(self):\n",
    "        data = pd.DataFrame({'text1': [1, 0.5, 0], 'text2': [0.5, 0.2, 1]})\n",
    "        lda_model = apply_lda(data)\n",
    "        self.assertIsInstance(lda_model, LatentDirichletAllocation)\n",
    "        \n",
    "    def test_get_topics(self):\n",
    "        data = pd.DataFrame({'text1': [1, 0.5, 0], 'text2': [0.5, 0.2, 1]})\n",
    "        vectorizer = CountVectorizer()\n",
    "        vectorizer.fit_transform(['hello world', 'python unit test'])\n",
    "        lda_model = LatentDirichletAllocation(n_components=2, random_state=123)\n",
    "        lda_model.fit(data)\n",
    "        topics = get_topics(lda_model, vectorizer)\n",
    "        self.assertIsInstance(topics, list)\n",
    "        \n",
    "    def test_sentiment_analysis(self):\n",
    "        data = pd.DataFrame({'lemmatized_clean': [['testing'], ['test']], 'oh_label': [1, 0]})\n",
    "        result, pivot_table_percentage = sentiment_analysis(data)\n",
    "        self.assertIn('sentiment_scores', result.columns)\n",
    "        self.assertIn('sentiment', result.columns)\n",
    "        self.assertIsInstance(pivot_table_percentage, pd.DataFrame)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18e7f331",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "........."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatisation successful\n",
      "Tokenisation successful.\n",
      "Most common words for oh_label = 1:\n",
      "[(\"['this',\", 1), (\"'is',\", 1), (\"'a']\", 1)]\n",
      "Most common words for oh_label = 0:\n",
      "[(\"['unit',\", 1), (\"'test',\", 1), (\"'prog']\", 1)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 9 tests in 0.256s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=9 errors=0 failures=0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suite = unittest.TestLoader().loadTestsFromTestCase(TestFunctions)\n",
    "runner = unittest.TextTestRunner()\n",
    "runner.run(suite)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
